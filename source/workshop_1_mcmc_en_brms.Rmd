---
title: "Workshop Bayesiaanse statistiek 2023"
subtitle: "1. MCMC, model specificatie met brms en interpretatie van de output"
author: "Raïsa Carmen, Ward Langeraert & Toon Van Daele"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# Set up
library(knitr)
library(here)
opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE)
opts_knit$set(root.dir = here())
```

```{r packages}
# Packages
library(GLMsData)  # datasets voor GLMs
library(tidyverse) # gegevensverwerking en visualisatie
library(brms)      # fitten van Bayesiaanse modellen
library(bayesplot) # MCMC visualisatie en posterior predictive checks

# Conflicten
conflicted::conflicts_prefer(dplyr::filter)
conflicted::conflicts_prefer(dplyr::lag)
conflicted::conflicts_prefer(brms::ar)
conflicted::conflicts_prefer(brms::dstudent_t)
conflicted::conflicts_prefer(brms::pstudent_t)
conflicted::conflicts_prefer(brms::qstudent_t)
conflicted::conflicts_prefer(brms::rstudent_t)
conflicted::conflict_prefer("rhat", "brms")
```

# Theoretische achtergrond

## Wat is Bayesiaanse statistiek?

In deze sectie herhalen we kort enkele basisprincipes van Bayesiaanse statistiek. 
We verwijzen ook graag naar het infomoment van 24 oktober 2023 (zie [infomoment 24/10/2023](https://drive.google.com/drive/folders/1MIWu8-LnhTUnKKAchjI7FEmaJ8TN_1mb)).



### Statistische inferentie

Stel dat we geïnteresseerd zijn in het aantal soorten mieren dat voorkomt in een onderzoekssite van een vooraf gespecificeerde grootte.
Aantallen worden vaak gespecificeerd door een Poisson verdeling aangezien deze verdeling enkel positieve gehele getallen aanneemt. 

$$ X \sim Poisson(\theta)$$
met $x$ het aantal soorten mieren en $\theta$ de parameter die we willen schatten.
In een Poisson verdeling is er slechts één parameter; $\theta$ is gelijk aan het gemiddelde en de variantie van de verdeling.
We kennen de echte waarde van $\theta$ niet *a priori*. 
Ons model definieert een collectie van waarschijnlijkheidsmodellen (probability models); één voor iedere mogelijke waarde van $\theta$. 
Deze collectie van modellen noemen we de *likelihood*.
In onderstaande figuur tonen we drie voorbeelden van een potentiele likelihood.
Het zijn drie kansdichtheidsfunctie van Poisson verdelingen met steeds een andere parameter $\theta$.

```{r}
likelihood <- data.frame(x = rep(seq(0, 25), 3),
                         theta = c(rep(5, 26), rep(10, 26), rep(15, 26))) %>%
  mutate(prob = stats::dpois(x, lambda = theta),
         theta = as.factor(theta))
likelihood %>%
  ggplot(aes(x = x, y = prob, color = theta)) +
  geom_point() + 
  geom_errorbar(aes(ymin = 0, ymax = prob), width = 0) + 
  xlab("aantal soorten mieren in een site") + 
  ylab("kansdichtheid")
```

Stel dat we op één bepaalde site 8 verschillende soorten mieren vinden.
Onze likelihood leert ons dat er een oneindig aantal waarden zijn voor $\theta$ waarmee we de uitkomst 8 krijgen. 
De figuur hieronder toont dat de kans op $X = 8$ verschillend is in ieder van de drie getoonde likelihoods.

```{r}
likelihood %>%
  ggplot(aes(x = x, y = prob, color = theta)) +
  geom_point() + 
  geom_errorbar(aes(ymin = 0, ymax = prob), width = 0) +
  geom_bar(data = likelihood %>% filter(x == 8),
           color = "grey", stat = "identity", alpha = 0.7) + 
  xlab("aantal soorten mieren in een site") + 
  ylab("kansdichtheid") +
  facet_grid(cols = vars(theta))
```

Ieder van deze modellen met steeds een andere waarde van $\theta$ kan ons de waarde van $x=8$ geven.
Bij statistische inferentie willen we de likelihood eigenlijk inverteren door gebruik te maken van onze voorkennis (prior).
Op deze manier hopen we te weten te komen welk model van alle potentiele modellen het meeste zinvol/waarschijnlijk is.
Zowel frequentist als bayesiaanse methoden inverteren in essentie $p(X|\theta) \rightarrow p(\theta|X)$ maar de methode verschilt.


**Frequentist inferentie**

Bij frequentist inferentie definiëren we een hypothese over het data-genererende proces die we willen testen:

-$H_0:$ de hypothese $\theta$ is waar
-$H_1:$ de hypothese $\theta$ is niet waar

De vuistregel in frequentist statistiek is dan:

- als $P($data $X$ of extremer$|\theta) < 0.05$, dan is \theta niet waar  $\Rightarrow  P(\theta|X) = 0$

- als $P($data $X$ of extremer$|\theta) \geq 0.05$, dan zou \theta kunnen waar zijn  $\Rightarrow P(\theta|X) = ?$

In onderstaande figuur staan de volgende kansen afgebeeld:

- $p(X\geq8|\theta = 5)\simeq$ `r 1 - round(likelihood %>% filter(x <= 8 & theta == 5) %>% dplyr::pull(prob) %>% sum(), 3)` Dit is groter dan 0.05 dus we kunnen de hypothese dat $\theta$ gelijk is aan 5 niet verwerpen.
- $p(X\leq8|\theta = 10)\simeq$ `r round(likelihood %>% filter(x <= 8 & theta == 10) %>% dplyr::pull(prob) %>% sum(), 3)` Dit is groter dan 0.05 dus we kunnen de hypothese dat $\theta$ gelijk is aan 10 niet verwerpen.
- $p(X\leq8|\theta = 15)\simeq$ `r round(likelihood %>% filter(x <= 8 & theta == 15) %>% dplyr::pull(prob) %>% sum(), 3)` Dit is kleiner dan 0.05 dus we verwerpen de hypothese dat $\theta$ gelijk is aan 15.

```{r}
likelihood %>%
  ggplot(aes(x = x, y = prob, color = theta)) +
  geom_point() + 
  geom_errorbar(aes(ymin = 0, ymax = prob), width = 0) +
  geom_bar(data = likelihood %>%
             filter((x <= 8 & theta %in% c(10,15))|
                      (x >= 8 & theta == 5)),
           color = "grey", stat = "identity", alpha = 0.7) + 
  xlab("aantal soorten mieren in een site") + 
  ylab("kansdichtheid") +
  facet_grid(cols = vars(theta))
```

Als we deze berekeningen herhalen voor verschillende waarden van $\theta$, kunnen we ontdekken dat het 90% confidence interval (5% langs beiden kanten) gelijk is aan $4.0 \leq \theta \leq 14.4$.
De beslissingsruimte voor de parameter $\theta$ wordt dus opgedeeld in twee delen; waarden die waarschijnlijk zijn en waarden die niet waarschijnlijk zijn.

**Bayesiaanse inferentie**

Bij Bayesiaanse inferentie wordt Bayes' rule gebruikt:

$$
p(\theta|X) = \frac{p(X|\theta) * p(\theta)}{p(X)}
$$
Dit zorgt er voor dat we voor iedere potentiele waarde van $\theta$ een waarschijnlijkheid zullen hebben.
De parameters hebben bij Bayesiaanse modellen dus een verdeling.
Dit staat in contrast met frequentist inferentie waar de oplossingsruimte voor $\theta$ opgesplitst wordt in twee delen: waarden die niet of wel waarschijnlijk zijn.

In ons voorbeeld van het aantal soorten mieren is:

- $\theta$ het gemiddelde aantal soorten mieren
- $X$ de data
- $p(X|\theta)$ de *likelihood*
- $p(\theta)$ de *prior*
- De noemer $p(X)$ maakt exacte Bayesiaanse inferentie moeilijk en heeft verschillende interpretaties:
  - Vooraleer we data verzamelen, is dit de *prior predictive distribution*
  - Als we data hebben, is dit een nummer dat de posterior normaliseert. Dit wordt *evidence* of *marginal likelihood* genoemd.
- $p(\theta|X)$ is de *posterior*. Het is een waarschijnlijkheidsverdeling. Deze posterior is het startpunt voor alle verdere analyse in Bayesiaanse inferentie.

https://benlambertdotcom.files.wordpress.com/2019/03/bayesian-course-1-short.pdf


Prior + likelihood -> posterior
normal/ binomial/ poisson

## Parameterschatting in Bayesiaanse statistiek

Herinner Bayes' rule:
$$
p(\theta|X) = \frac{p(X|\theta) * p(\theta)}{p(X)}
$$

We vermeldden eerder reeds dat de noemer hier heel moeilijk is om te berekenen.
Er zijn twee mogelijke oplossingen:

- *Conjugate priors*: Voor sommige verdelingen en combinaties van priors en likelihoods is het mogelijk om toch een analytische oplossing te krijgen (zie [deze lijst](https://en.wikipedia.org/wiki/Conjugate_prior)). Dit beperkt echter onze mogelijkheden:
  - enkel mogelijk voor enkele univariate en bivariate problemen
  - de conjugate priors beperken de vrijheid om je voorkennis voor te stellen. (bijvoorbeeld zero-inflation, )
- *Sampling*: we krijgen informatie over de verdeling door eruit te samplen in plaats van het exact te berekenen.

In Bayesiaanse inferentie zijn er verschillende mogelijkheden voor puntschattingen:

- het posterior gemiddelde
- de posterior mediaan
- maximum a posteriori (MAP) oftewel de mode. De mode kan soms ver van het gemiddelde en de mediaan liggen.

Ook voor de weergave van onzekerheid zijn er verschillende opties:

- Highest posterior density interval (HPDI): dit is interessant als het bijvoorbeeld belangrijk is om regio's met een lage densiteit te vermijden.
- Equal-tailed credible interval = Central posterior interval (CPI): Voor de berekening van een ($1-\alpha$)% interval, zal er $\frac{\alpha}{2}$ in zowel de linker als rechter "staart" van de verdeling zitten.

Bij symmetrische, unimodale vverdelingen zijn HPDI en CPI hetzelfde (zie eerste figuur hieronder).
Anders kan er best wel wat verschil zitten tussen beide intervallen (zie figuur 2 en 3 hieronder).

```{r, purl = FALSE}
# specify a unimodal, symmetric distribution
unimodal <- data.frame(
  x = seq(10, 50)
) %>%
  mutate(prob = dnorm(x, mean = 30, sd = 5),
        cum_sum = pnorm(x, mean = 30, sd = 5))
limits <- data.frame(type = c("CPI", "CPI", "HPDI", "HPDI"),
                     x = c(13, 46, 13, 46))

p1 <- ggplot(unimodal) +
  geom_line(aes(x = x, y = prob)) +
  geom_vline(data = limits, aes(xintercept = x, color = type, lty = type),
             alpha = 0.5)

# specify a bimodal distribution
bimodal <- data.frame(
  x = seq(0, 50, 0.1)
) %>%
  mutate(prob = (dnorm(x, mean = 32, sd = 4) +
           dnorm(x, mean = 12, sd = 2))/2,
        cum_sum = (pnorm(x, mean = 32, sd = 4) + pnorm(x, mean = 12, sd = 2))/2)
measures <- data.frame(measures = c("mean", "mode", "median"),
                       x = c(weighted.mean(x = bimodal$x, w = bimodal$prob),
                             12, 18.7))
limits <- data.frame(type = c("CPI", "CPI", "HPDI", "HPDI", "HPDI", "HPDI"),
                     x = c(9.5, 37, 7.9, 16.1, 25.4, 38.5))
p2 <- ggplot(bimodal) +
  geom_vline(data = limits %>% filter(type == "HPDI"), aes(xintercept = x)) +
  geom_bar(data = bimodal %>%
             filter((x >= 7.9 & x <= 16.1) | (x >= 25.4 & x <= 38.5)),
           aes(x = x, y = prob),
           stat = "identity", color = "grey", fill = "grey") +
  ggtitle("highest posterior density interval for a bimodal distribution") +
  geom_line(aes(x = x, y = prob)) + 
  geom_vline(data = measures, aes(xintercept = x), color = "red") +
  geom_text(data = measures,
            aes(label = measures, x = x, y = c(0.1, 0.09, 0.08)), color = "red")
p3 <- ggplot(bimodal) +
  geom_vline(data = limits %>% filter(type == "CPI"), aes(xintercept = x)) +
  geom_bar(data = bimodal %>% filter(x >= 9.5 & x <= 37),
           aes(x = x, y = prob),
           stat = "identity", color = "grey", fill = "grey") +
  ggtitle("equal tail credible interval for a bimodal distribution") +
  geom_line(aes(x = x, y = prob))
p1 
```

```{r, purl = FALSE}
p2
```
```{r, purl = FALSE}
p3
```

laat ze zelf een chain runnen (buiten brms?)
Chains, iterations, burn-in -> https://nicercode.github.io/guides/mcmc/
Rhat
Parallel running
Stan
Verschillende regels metropolis-hastings algoritme
MCMC: vermeld verdunning (thinning), chains, burn-in, iterations

output: posterior, hpdi, credible interval, 

# Een model fitten in brms
## Dataset laden en data exploratie

We laden een dataset in van het aantal mierensoorten in New England (USA).
Typ `?ants` in de console voor meer info.

```{r}
# Laad dataset in
data(ants)

# Maak kopie met kolomnamen in kleine letter
ants_df <- ants %>%
  rename(sp_rich = Srich) %>%
  rename_with(tolower)

# Hoe zien de data eruit?
glimpse(ants_df)
```

Enkele samenvattende statistieken:

```{r}
summary(ants_df)
```

We visualiseren de data.

```{r}
ants_df %>%
  ggplot(aes(x = sp_rich)) +
    geom_bar() +
    scale_x_continuous(limits = c(0, NA)) +
    facet_wrap(~habitat)
```

```{r}
ants_df %>%
  ggplot(aes(y = sp_rich, x = habitat)) +
    geom_boxplot() +
    scale_y_continuous(limits = c(0, NA))
```

```{r}
ants_df %>%
  ggplot(aes(y = sp_rich, x = latitude)) +
    geom_point() +
    geom_smooth(method = "loess", formula = "y ~ x", colour = "firebrick") +
    facet_wrap(~habitat)
```

```{r}
ants_df %>%
  ggplot(aes(y = sp_rich, x = elevation)) +
    geom_point() +
    geom_smooth(method = "loess", formula = "y ~ x", colour = "firebrick") +
    facet_wrap(~habitat)
```


## Een eerste simpel model
### Model specificatie

Eerst en vooral specificeren we de MCMC parameters ...

```{r}
# MCMC parameters
nchains <- 3           # aantal chains
niter <- 2000          # aantal iteraties (incl. burn-in, zie volgende)
burnin <- niter / 4    # aantal initiële samples om te verwijderen (= burn-in)
nparallel <- nchains   # aantal cores voor parallel computing
thinning <- 1          # verdunningsfactor (hier 1 =geen verdunning)
```

$$
Y \sim Pois(\beta_0 + \beta_1X_{habitat})\\
\Rightarrow \ln(E(Y)) = \beta_0 + \beta_1X_{habitat}
$$

```{r}
# Fit Poisson model
fit1 <- brm(formula = bf(sp_rich ~ habitat),
            data = ants_df,
            family = poisson(),
            chains = nchains, 
            warmup = burnin, 
            iter = niter,
            cores = nparallel,
            thin = thinning,
            seed = 123)
```


### MCMC convergentie

https://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html

```{r}
color_scheme_set("mix-blue-red")
```

Wat is een trace plot?

```{r}
mcmc_trace(fit1, pars = c("b_Intercept", "b_habitatForest"))
```

Wat is posterior density?

```{r}
mcmc_dens_overlay(fit1, pars = c("b_Intercept", "b_habitatForest"))
```

Of eenvoudiger:

```{r}
plot(fit1)
```


Wat is Rhat?

```{r}
rhats_fit1 <- rhat(fit1)[c("b_Intercept", "b_habitatForest")]
mcmc_rhat(rhats_fit1) + yaxis_text(hjust = 1)
```

+ effective sample size
...

Correlatie?...

```{r}
mcmc_pairs(fit1, pars = c("b_Intercept", "b_habitatForest"),
           off_diag_args = list(size = 1.5))
```

### Model fit

https://mc-stan.org/bayesplot/articles/graphical-ppcs.html

Uitleg posterior predictive check.

```{r}
pp_check(fit1, type = "dens_overlay_grouped", ndraws = 100, 
         group = "habitat")
```

# Vergelijken van modellen
## Model specificatie

```{r}
fit2 <- brm(formula = bf(sp_rich ~ habitat + (1|site)),
            data = ants_df,
            family = poisson(),
            chains = nchains, 
            warmup = burnin, 
            iter = niter,
            cores = nparallel,
            thin = thinning,
            seed = 123)
```

## MCMC convergentie

```{r}
plot(fit2)
```

```{r}
vars <- c("b_Intercept", "b_habitatForest", "sd_site__Intercept")
rhats_fit2 <- rhat(fit2)[vars]
mcmc_rhat(rhats_fit2) + yaxis_text(hjust = 1)
```

## Model fit

```{r}
pp_check(fit2, type = "dens_overlay_grouped", ndraws = 100, 
         group = "habitat")
```

## Model selectie

https://mc-stan.org/loo/articles/online-only/faq.html

```{r}
# Leave-one-out cross-validation
fit1 <- add_criterion(fit1, criterion = "loo")
fit2 <- add_criterion(fit2, criterion = "loo")

# Maak vergelijking
comp <- loo_compare(fit1, fit2, criterion = "loo")
print(comp, simplify = FALSE, digits = 3)
```

Vergelijking met onzekerheid:

```{r}
comp %>%
  as.data.frame() %>%
  select(elpd_diff, se_diff) %>%
  mutate(ll_diff = elpd_diff  + qnorm(0.025) * se_diff,
         ul_diff = elpd_diff  + qnorm(0.975) * se_diff)
```

# Resultaten finale model

https://mjskay.github.io/tidybayes/articles/tidy-brms.html

```{r}
fit2 %>%
  gather_draws(b_Intercept, b_habitatForest, ndraws = 1000, seed = 123) %>%
  group_by(.variable) %>%
  summarise(min = min(.value),
            q_05 = quantile(.value, probs = 0.05),
            gemiddelde = mean(.value),
            mediaan = median(.value),
            q_95 = quantile(.value, probs = 0.95),
            max = max(.value))
```

Handige functies zijn ook `median_qi()`, `mean_qi()` ...  na `gather_draws()` in plaats van `group_by()` en `summarise()`.

We zouden graag het geschatte aantal soorten visualiseren per habitattype.
Het gemiddeld aantal soorten in moerassen volgens ons model is $\exp(\beta_0)$ en in bossen $\exp(\beta_0+\beta_1)$.

```{r}
plot_df <- fit2 %>%
  spread_draws(b_Intercept, b_habitatForest, ndraws = 1000, seed = 123) %>%
  mutate(bog = exp(b_Intercept),
         forest = exp(b_Intercept + b_habitatForest))
```

We tonen ook de posterior mediaan en 60 en 90 % credible interval.

```{r}
plot_df %>%
  pivot_longer(cols = c("bog", "forest"), names_to = "habitat", 
               values_to = "sp_rich") %>%
  ggplot(aes(y = sp_rich, x = habitat)) +
    stat_eye(point_interval = "median_qi", .width = c(0.6, 0.9)) +
    scale_y_continuous(limits = c(0, NA))
```

We zien een duidelijk verschil in aantal soorten tussen beide habitats.
Is er een significant verschil tussen het aantal soorten in moerassen en bossen?
We testen de hypothese

$$
\exp(\beta_0) = \exp(\beta_0+\beta_1)\\
\Rightarrow \beta_0 = \beta_0 + \beta_1\\
\Rightarrow \beta_1 = 0\\
$$

```{r}
hyp <- hypothesis(fit2, "habitatForest = 0", alpha = 0.05)
hyp
```

```{r}
plot(hyp)
```

We sorteren de random effecten van de sites.

```{r}
# Get mean of sd of random effects 
sd_mean <- fit2 %>%
  spread_draws(sd_site__Intercept, ndraws = 1000, seed = 123) %>%
  summarise(mean_sd = mean(sd_site__Intercept)) %>%
  pull()

# Get random effects and plot
fit2 %>%
  spread_draws(r_site[site,], ndraws = 1000, seed = 123) %>%
  ungroup() %>%
  mutate(site = reorder(site, r_site)) %>%
  ggplot(aes(x = r_site, y = site)) +
    geom_vline(xintercept = 0, color = "darkgrey", linewidth = 1) +
    geom_vline(xintercept = c(sd_mean * qnorm(0.025), sd_mean * qnorm(0.975)),
               color = "darkgrey", linetype = 2) +
    stat_halfeye(point_interval = "median_qi", .width = 0.9, size = 2/3,
                 fill = "cornflowerblue")
```
