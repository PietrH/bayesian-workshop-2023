---
title: "Workshop Bayesiaanse statistiek 2023"
subtitle: "1. MCMC, model specificatie met brms en interpretatie van de output"
author: "Raïsa Carmen, Ward Langeraert & Toon Van Daele"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    code_folding: show
    toc: true
    toc_float: true
    toc_collapsed: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
# Set up
library(knitr)
library(here)
opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE)
opts_knit$set(root.dir = here())
```

```{r packages}
# Packages
library(GLMsData)  # datasets voor GLMs
library(tidyverse) # gegevensverwerking en visualisatie
library(brms)      # fitten van Bayesiaanse modellen
library(bayesplot) # MCMC visualisatie en posterior predictive checks
library(tidybayes) # nabewerking en visualisatie van Bayesiaanse modellen

# Conflicten
conflicted::conflicts_prefer(dplyr::filter)
conflicted::conflicts_prefer(dplyr::lag)
conflicted::conflicts_prefer(brms::ar)
conflicted::conflicts_prefer(brms::dstudent_t)
conflicted::conflicts_prefer(brms::pstudent_t)
conflicted::conflicts_prefer(brms::qstudent_t)
conflicted::conflicts_prefer(brms::rstudent_t)
conflicted::conflict_prefer("rhat", "brms")
```

# Wat is Bayesiaanse statistiek?

Korte herhaling: wat is bayesiaanse statistiek
Prior + likelihood -> posterior
normal/ binomial/ poisson

# Parmeterschatting via MCMC

laat ze zelf een chain runnen (buiten brms?)
Chains, iterations, burn-in -> https://nicercode.github.io/guides/mcmc/
Rhat
Parallel running
Stan
Verschillende regels metropolis-hastings algoritme

# Een model fitten met brms
## Dataset laden en data exploratie

We laden een dataset in over het aantal mierensoorten in New England (USA).
Typ `?ants` in de console voor meer info.

```{r}
# Laad dataset in
data(ants)

# Maak kopie met kolomnamen in kleine letters
ants_df <- ants %>%
  rename(sp_rich = Srich) %>%
  rename_with(tolower)

# Hoe zien de data eruit?
glimpse(ants_df)
```

We hebben een aantal sites (`site`) waar het aantal soorten mieren geteld zijn (`sp_rich`).
Enkele variabelen zijn aanwezig: `habitat`, `latitude` en hoogte (`elevation`).
We bekijken enkele samenvattende statistieken.
In elke site zijn 2 tellingen gedaan: in moeras (`bog`) en in bos (`forest`).

```{r}
summary(ants_df)
```

We visualiseren de data.

Frequentie van het aantal soorten per habitat.

```{r}
ants_df %>%
  ggplot(aes(x = sp_rich)) +
    geom_bar() +
    scale_x_continuous(limits = c(0, NA)) +
    facet_wrap(~habitat)
```

Verdeling van het aantal soorten per habitat.

```{r}
ants_df %>%
  ggplot(aes(y = sp_rich, x = habitat)) +
    geom_boxplot() +
    geom_line(aes(colour = site, group = site), alpha = 0.5) +
    geom_point(aes(colour = site, group = site)) +
    scale_y_continuous(limits = c(0, NA))
```

Relatie tussen het aantal soorten en latitude per habitat.

```{r}
ants_df %>%
  ggplot(aes(y = sp_rich, x = latitude)) +
    geom_point() +
    geom_smooth(method = "loess", formula = "y ~ x", colour = "firebrick") +
    facet_wrap(~habitat)
```

Relatie tussen het aantal soorten en hoogte per habitat.

```{r}
ants_df %>%
  ggplot(aes(y = sp_rich, x = elevation)) +
    geom_point() +
    geom_smooth(method = "loess", formula = "y ~ x", colour = "firebrick") +
    facet_wrap(~habitat)
```

Als oefening zullen we een model maken om het aantal soorten te vergelijken tussen beide habitats.
Uit de data exploratie zagen we al dat het aantal hoger lijkt te liggen in bossen en dat sites met een hoger aantal in moerassen vaak ook een hoger aantal in bossen hebben.

## Specificatie van een lineaire regressie
### Model specificatie

We willen een model waarmee we het verschil kunnen onderzoeken in aantal soorten mieren tussen moeras- en boshabitats.
Beschouw de responsvariabele $Y$ het aantal mieren en $X_{habitat}$ een dummy variabele die gelijk is aan 0 voor moerassen en 1 voor bossen.
We veronderstellen dat $Y$ een Normaal verdeling volgt (equivalent aan een lineaire regressie met categorische variabele (= ANOVA))

$$
Y \sim N(\mu, \sigma^2)
$$

met

$$
\mu = \beta_0 + \beta_1X_{habitat}
$$

We moeten dus drie parameters schatten: $\beta_0$, $\beta_1$ en $\sigma$.
Hoe specificeren we dit in **brms**?

Eerst en vooral besluiten we welke MCMC parameters we zullen gebruiken.
Typ `?brm` om te zien wat de standaard instellingen zijn voor deze parameters.
Aangeraden is om meerdere chains te gebruiken (`nchains`) en deze parallel te laten lopen op verschillende cores van je computer (`nparallel`).
Aangezien dat dit een relatief simpel model is, hebben we niet veel iteraties nodig en geen verdunning.

```{r}
# MCMC parameters
nchains <- 3           # aantal chains
niter <- 2000          # aantal iteraties (incl. burn-in, zie volgende)
burnin <- niter / 4    # aantal initiële samples om te verwijderen (= burn-in)
nparallel <- nchains   # aantal cores voor parallel computing
thinning <- 1          # verdunningsfactor (hier 1 = geen verdunning)
```

Het model wordt gefit a.d.h.v. de `brm()` functie.
De syntax is zeer gelijkaardig als functies die in frequentist statistics worden gebruikt zoals `lm()` en `glm()`.
De `brm()` functie bevat heel wat argumenten (zie `?brm`).
Handige argumenten die we hier niet gebruiken zijn bijvoorbeeld:

-  `inits` om de beginwaarden van MCMC algoritme in te stellen. Indien deze niet gespecificeerd zijn, gebruikt de functie default waarden.
-  `prior` om prior distributies te voorzien. Indien deze niet gespecificeerd zijn, gebruikt de functie default (niet informatieve) priors. `sample_prior` kan gebruikt worden om van de prior te samplen zodat je kan visualiseren of de gebruikte prior voldoet aan je verwachtingen.
-  `file` en `file_refit` om het model object op te slaan nadat het gefit is. Als je de code opnieuw runt en het model is al eens opgeslaan, dan zal `brm()` dit model gewoon inladen in plaats van het opnieuw te fitten.

```{r}
# Fit Normaal model
fit_normal1 <- brm(
  formula = sp_rich ~ habitat, # beschrijving van het model
  family = gaussian(),         # we gebruiken de Normaal verdeling
  data = ants_df,              # ingeven data
  chains = nchains,            # MCMC parameters
  warmup = burnin, 
  iter = niter,
  cores = nparallel,
  thin = thinning,
  seed = 123)                  # seed voor reproduceerbare uitkomst
```

Voor we de resultaten bekijken, controleren we eerst of het model goed geconvergeerd is.

### MCMC convergentie

Er zijn verschillende manieren om convergentie van het MCMC algoritme voor elke parameter te controleren.
Hierbij worden de burn-in samples niet in rekening genomen.
Eerst en vooral heb je *visuele controles*.

We kunnen de MCMC samples met de `as_draws()` functies verkrijgen ofwel ineens visualiseren via de [bayesplot](https://mc-stan.org/bayesplot/) package die compatibel is met brmsfit objecten.

```{r}
# Zet kleurenpallet voor duidelijke visualisatie in bayesplot
color_scheme_set("mix-blue-red")
```

```{r}
# Welke parameters gaan we bekijken?
parameters <- c("b_Intercept", "b_habitatForest", "sigma")
```

**trace plot**

Trace plots geven aan hoe het samplen van de posterior distribution verloopt over de tijd. Het idee is dat elke chain convergeert naar dezelfde dezelfde distributie voor elke parameter (we spreken van mixing). Als deze plot een harige rups ('fuzzy caterpillar') voorstelt, wil dit zeggen dat convergentie goed is.

```{r}
# Visualisatie door extractie samples met as_draws_df()
as_draws_df(fit_normal1, variable = parameters) %>%
  pivot_longer(cols = all_of(parameters), names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(y = value, x = .iteration, colour = factor(.chain))) +
    geom_line() +
    facet_wrap(~parameter, nrow = 3, scales = "free")
```

```{r}
# Visualisatie via Bayesplot package
mcmc_trace(fit_normal1, pars = parameters)
```

**running mean/quantile/... plot**

Gelijkaardig als een trace plot kunnen we ook kijken hoe bepaalde statistieken zoals het gemiddelde of de quantielen zich voordoen over de tijd (= met toename aantal iteraties). Je wilt natuurlijk dat deze statistieken ook constant zijn na een aantal iteraties. Dan weet je dat je genoeg iteraties gebruikt hebt.

```{r}
# code om cumulatieve quantielen te berekenen
# bron: https://rdrr.io/cran/cumstats/src/R/cumquant.R
cumquant <- function(x, p) {
  out <- sapply(seq_along(x),
                function(k, z) quantile(z[1:k], probs = p[1], names = FALSE),
                z = x)
  return(out)
}
```

```{r}
as_draws_df(fit_normal1, variable = parameters) %>%
  mutate(across(all_of(parameters), ~cummean(.x),
                .names = "{.col}_gemiddelde"),
         across(all_of(parameters), ~cumquant(.x, 0.1),
                .names = "{.col}_q10"),
         across(all_of(parameters), ~cumquant(.x, 0.9),
                .names = "{.col}_q90")) %>%
  select(-all_of(parameters)) %>%
  pivot_longer(cols = starts_with(parameters), names_to = "name",
               values_to = "value") %>%
  extract(name, into = c("parameter", "statistiek"), "(.*)_([^_]+)$") %>%
  ggplot(aes(y = value, x = .iteration, colour = factor(.chain))) +
    geom_line(aes(linetype = statistiek), linewidth = 0.8) +
    facet_wrap(~parameter, nrow = 3, scales = "free")
```

**posterior density plot**

De vorm van de posterior distributions kan ook informatief zijn of de chains geconvergeerd zijn naar dezelfde distributies.
Bovendien is het bijvoorbeeld mogelijk dat er meerdere maxima aanwezig zijn.
Dan zou er mogelijks ook iets mis kunnen zijn met de specificatie van ons model.

```{r}
mcmc_dens_overlay(fit_normal1, pars = parameters)
```

Nog eenvoudiger is de `plot()` functie gebruiken op het brmsfit object.
Deze toont direct de trace plots en de posterior density plots naast elkaar voor elke parameter.

```{r}
plot(fit_normal1)
```

**autocorrelation plot**

> uitleg Wat is autocorrelatie? thinning

```{r}
mcmc_acf(fit_normal1, pars = parameters)
```

**cross-correlation plot**

> uitleg Wat is cross-correlatie?

```{r}
mcmc_pairs(fit_normal1, pars = parameters,
           off_diag_args = list(size = 1.5))
```

Naast visuele checks zijn er ook *diagnostische controles*.

**Gelman-Rubin diagnostic**

Ook wel $\hat{R}$ (R-hat) of potential scale reduction factor genoemd.
Een manier om te controleren of een chain is geconvergeerd, is door zijn gedrag te vergelijken met andere willekeurig geïnitialiseerde ketens.
De $\hat{R}$ statistiek neemt de verhouding van de gemiddelde variantie van samples binnen elke chain tot de variantie van de gepoolde samples over alle chains heen.
Als alle ketens convergeren naar een gemeenschappelijke distributie, zullen deze verhoudingen gelijk aan 1 zijn. Als de ketens niet zijn geconvergeerd naar een gemeenschappelijke verdeling, zal de $\hat{R}$ statistiek groter zijn dan 1.
We kunnen de $\hat{R}$ statistieken extraheren via de `rhat()` functie en dan eventueel visualiseren met de `mcmc_rhat()` functie van de **bayesplot** package.

```{r}
# Krijg R-hats
rhats_fit_normal1 <- rhat(fit_normal1)[parameters]
print(rhats_fit_normal1)
```

```{r}
# Plot R-hats
mcmc_rhat(rhats_fit_normal1) + yaxis_text(hjust = 1)
```

**effective sample size**

De effective sample size ($n_{eff}$) is een schatting van het aantal onafhankelijke trekkingen van de posterior distribution van elke parameter. Omdat de trekkingen binnen een Markov chain niet onafhankelijk zijn als er sprake is van autocorrelatie, is de effective sample size gewoonlijk kleiner dan de totale steekproefomvang, namelijk het aantal iteraties ($N$). Hoe groter de verhouding $n_{eff}/N$ hoe beter. De **bayesplot** package biedt een `neff_ratio()` extractorfunctie. De `mcmc_neff()` functie kan vervolgens worden gebruikt om de verhoudingen te plotten.

```{r}
# Krijg verhoudingen
ratios_fit_normal1 <- neff_ratio(fit_normal1)[parameters]
print(ratios_fit_normal1)
```

```{r}
# Plot verhoudingen
mcmc_neff(ratios_fit_normal1) + yaxis_text(hjust = 1)
```

Andere packages die van pas kunnen komen bij het controleren van MCMC convergentie zijn [mcmcplots](https://cran.r-project.org/web/packages/mcmcplots/index.html) en [coda](https://cran.r-project.org/web/packages/coda/index.html).
Voor deze packages moet je de MCMC samples wel eerst omzetten naar een mcmc object.

```
# voorbeeld
install.packages(mcmcplots)
mcmcplots::mcmcplot(as.mcmc(fit_normal1, pars = parameters))
```

We kunnen over het algemeen besluiten dat MCMC convergentie goed is voor alle parameters.
Indien dit niet het geval zou zijn, kan het zijn dat de MCMC parameters anders moeten gekozen worden, bijvoorbeeld door het aantal iteraties te verhogen.
Nu we weten dat de parameters correct geschat zijn, kunnen we controleren of het model goed bij de data past.

### Model fit

De [posterior predictive check](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html) (PPC) is een goede manier om te controleren of het model goed past bij de data.
Het idee achter de PPC is dat, als een model goed past, de voorspelde waardes o.b.v. het model veel lijken op de data die we hebben gebruikt om het model te fitten.
Om de voorspellingen te genereren die worden gebruikt voor PPCs, simuleren we vanuit de posterior predictive distribution (de posterior distribution van de responsvariabele).
Visualisatie kan met de **bayesplot** package.
De dikke lijn geeft de data aan en de dunne lijntjes verschillende simulaties o.b.v. het model.

```{r}
pp_check(fit_normal1, type = "dens_overlay_grouped", ndraws = 100, 
         group = "habitat")
```

We zien dat de overeenkomst tussen de geobserveerde data en de voorspellingen o.b.v. ons model niet helemaal overeenkomen.
Mogelijks was onze keuze voor de Normaal verdeling niet goed.
Aantallen zijn discrete, positieve waarden, terwijl de Normaal verdeling continue en zowel positieve als negatieve waarden kan voorspellen.
De Poisson verdeling is een discrete verdeling die steeds positief is.
Ze wordt vaak gebruikt voor het modelleren van geregistreerde aantallen gedurende een gegeven tijdsinterval, afstand, oppervlakte, volume ...

---
**Opmerking**

Merk op dat MCMC convergentie en model fit twee verschillende dingen zijn. Het is niet omdat MCMC convergentie goed is dat model fit ook goed is. MCMC convergentie is om na te gaan of het MCMC algoritme om de distributies van de parameters te schatten goed verlopen is. Model fit controleert of het model dat door ons aangenomen is (assumptie normaliteit, lineair verband gemiddelde ...) goed past bij de data.

---

## Specificatie van een Poisson model
### Model specificatie

We veronderstellen dat $Y$ nu een Poisson verdeling volgt

$$
Y \sim Pois(\lambda)
$$

met

$$
\ln(\lambda) = \beta_0 + \beta_1X_{habitat}
$$

We moeten dus twee parameters schatten: $\beta_0$ en $\beta_1$ 
We gebruiken dezelfde MCMC parameters als voordien.
Het enige wat we moeten aanpassen is de keuze `family = poisson()`.

```{r}
# Fit Poisson model
fit_poisson1 <- brm(
  formula = sp_rich ~ habitat, # beschrijving van het model
  family = poisson(),          # we gebruiken de Poisson verdeling
  data = ants_df,              # ingeven data
  chains = nchains,            # MCMC parameters
  warmup = burnin, 
  iter = niter,
  cores = nparallel,
  thin = thinning,
  seed = 123)                  # seed voor reproduceerbare uitkomst
```

### MCMC convergentie

Convergentie ziet er opnieuw goed uit.

```{r}
plot(fit_poisson1)
```

```{r}
rhats_fit_poisson1 <- rhat(fit_poisson1)[c("b_Intercept", "b_habitatForest")]
mcmc_rhat(rhats_fit_poisson1) + yaxis_text(hjust = 1)
```

### Model fit

We zien dat alle predicties nu wel positief zijn, maar de fit is nog altijd niet ideaal.

```{r}
pp_check(fit_poisson1, type = "dens_overlay_grouped", ndraws = 100, 
         group = "habitat")
```

### Discussie

Hoe kunnen we model fit nog verbeteren?

We weten dat elke site 2x bezocht is.
Eenmaal in moeras en eenmaal in bos.
Het is natuurlijk mogelijk dat het aantal mieren in het moeras en het bos van dezelfde site gecorreleerd zullen zijn (site effect).
Dit zagen we inderdaad ook in de data exploratie.
Sites met een hoger aantal soorten in moerassen hebben vaak ook een hoger aantal in bossen en vice versa.
Hiervoor kunnen we corrigeren door een random intercept voor elke site toe te voegen `... + (1|site)`.

We veronderstellen dat het aantal soorten $Y$ per site $j$ ($j = 1, ..., J$) een Poisson verdeling volgt

$$
Y_j \sim Pois(\lambda_j)
$$

zodat

$$
\ln(\lambda_j) = \beta_0 + \beta_1X_{habitat} + b_{0,j}
$$

met 

$$
b_0 \sim N(0, \sigma_b)
$$

```{r}
# Fit Poisson model met random intercepten per site
fit_poisson2 <- brm(
  formula = sp_rich ~ habitat + (1|site),
  family = poisson(),
  data = ants_df,
  chains = nchains,
  warmup = burnin, 
  iter = niter,
  cores = nparallel,
  thin = thinning,
  seed = 123)
```

Convergentie is goed.

```{r}
plot(fit_poisson2)
```

```{r}
parameters2 <- c("b_Intercept", "b_habitatForest", "sd_site__Intercept")
rhats_fit_poisson2 <- rhat(fit_poisson2)[parameters2]
mcmc_rhat(rhats_fit_poisson2) + yaxis_text(hjust = 1)
```

Model fit lijkt nu nog beter.

```{r}
pp_check(fit_poisson2, type = "dens_overlay_grouped", ndraws = 100, 
         group = "habitat")
```

Hoe kunnen we deze modellen nu objectief gaan vergelijken?

# Vergelijken van modellen

Op basis van de PPCs kunnen we reeds zien welk model het best past bij de data.
Verder zijn er nog enkele functies die **brms** voorziet om verschillende modellen te vergelijken.
Met de functie `add_criterion()` kan je model fit criteria toevoegen aan model objecten.
Typ `?add_criterion()` om te zien welke beschikbaar zijn.
Zie ook <https://mc-stan.org/loo/articles/online-only/faq.html>

## Leave-one-out cross-validation

Cross-validation CV is een familie van technieken die probeert in te schatten hoe goed een model onbekende data zou voorspellen via predicties van het model gefit op de bekende data.
Hiervoor moet je niet per se nieuwe data gaan inzamelen.
Je kan jouw eigen data opsplitsen in een test en training dataset. 
Je fit het model op de training dataset en je gebruikt dat model dan om te schatten hoe goed het de data in de test dataset kan voorspellen.
Bij leave-one-out CV ga je telkens één observatie weglaten en het model opnieuw fitten o.b.v. alle andere data.

```{r}
# Voeg model fit criteria toe aan model objecten
fit_normal1 <- add_criterion(fit_normal1,
                             criterion = c("loo"))
fit_poisson1 <- add_criterion(fit_poisson1,
                              criterion = c("loo"))
fit_poisson2 <- add_criterion(fit_poisson2,
                              criterion = c("loo"))
```

Om het verschil tussen de voorspelling van het model o.b.v. de training dataset te vergelijken met de weggelaten data, moeten we een zgn. utitily of loss function definiëren.
Wij gebruiken hier de 'expected log pointwise predictive density' (ELPD).
Voor deze workshop is het vooral van belang te beseffen dat dit een maat is voor hoe goed jouw Bayesiaanse model nieuwe datapunten voorspelt.
Het wordt berekend door de log-likelihood van elk datapunt te nemen uit de posterior predictive distribution van je model en deze vervolgens uit te middelen. Een hogere elpd duidt op een betere modelfit en voorspellende nauwkeurigheid.
Voor deze workshop is het vooral van belang te beseffen dat dit een maat is voor hoe goed het model onbekende data kan voorspellen (`elpd_loo` en de standaard error `se_elpd_loo`).
`p_loo` is een maat voor de complexiteit van het model.
`looic` =  -2*`elpd_loo`
Met de functie `loo_compare()` kan je meerdere modellen vergelijken en wordt ook het verschil in ELPD berekend.

```{r}
# Maak vergelijking leave-one-out cross-validation
comp_loo <- loo_compare(fit_normal1, fit_poisson1, fit_poisson2,
                        criterion = "loo")
print(comp_loo, simplify = FALSE, digits = 3)
```

Als je ervan uitgaat dat dit verschil normaal verdeeld is, kan je met de onzekerheid een betrouwbaarheidsinterval berekenen.
We zien dat het tweede Poisson model het best scoort en dat de andere modellen significant lager scoren.

```{r}
comp_loo %>%
  as.data.frame() %>%
  select(elpd_diff, se_diff) %>%
  mutate(ll_diff = elpd_diff  + qnorm(0.025) * se_diff,
         ul_diff = elpd_diff  + qnorm(0.975) * se_diff)
```

## K-fold cross-validation

Bij K-fold cross-validation wordt de data in $K$ groepen opgesplitst.
Wij zullen hier $K = 10$ groepen (= folds) gebruiken.
In plaats van dus telkens één enkele observatie weg te laten zoals bij leave-one-out CV gaan we hier $1/10$e van de data weglaten.
Via de argumenten `folds = "stratified"` en `group = "habitat"` zorgen we ervoor dat voor elke groep de relatieve frequenties van habitat bewaard blijven.
Deze techniek zal dus minder precies zijn dan de vorige, maar zal sneller zijn om te berekenen indien je met heel veel data werkt.

```{r}
# Voeg model fit criteria toe aan model objecten
fit_normal1 <- add_criterion(fit_normal1,
                             criterion = c("kfold"),
                             K = 10,
                             folds = "stratified",
                             group = "habitat")
fit_poisson1 <- add_criterion(fit_poisson1,
                              criterion = c("kfold"),
                              K = 10,
                              folds = "stratified",
                              group = "habitat")
fit_poisson2 <- add_criterion(fit_poisson2,
                              criterion = c("kfold"),
                              K = 10,
                              folds = "stratified",
                              group = "habitat")
```

We krijgen dezelfde statistieken terug als voordien.

```{r}
# Maak vergelijking k-fold cross-validation
comp_kfold <- loo_compare(fit_normal1, fit_poisson1, fit_poisson2,
                          criterion = "kfold")
print(comp_kfold, simplify = FALSE, digits = 3)
```

De resultaten zijn hetzelfde, maar de verschillen zijn iets kleiner als voordien.
Het verschil tussen de twee Poisson modellen is niet meer significant.

```{r}
comp_kfold %>%
  as.data.frame() %>%
  select(elpd_diff, se_diff) %>%
  mutate(ll_diff = elpd_diff  + qnorm(0.025) * se_diff,
         ul_diff = elpd_diff  + qnorm(0.975) * se_diff)
```

## WAIC

Het Widely Applicable Information Criterion (WAIC) maakt geen gebruik van cross-validation maar is een computationele manier om de ELPD te schatten.
Hoe dit precies gebeurt valt buiten het doel van deze workshop.
Het is nog een andere maat om model selectie toe te passen.

```{r}
# Voeg model fit criteria toe aan model objecten
fit_normal1 <- add_criterion(fit_normal1,
                             criterion = c("waic"))
fit_poisson1 <- add_criterion(fit_poisson1,
                              criterion = c("waic"))
fit_poisson2 <- add_criterion(fit_poisson2,
                              criterion = c("waic"))
```

We krijgen opnieuw gelijkaardige resultaten als voordien.

```{r}
# Maak vergelijking waic
comp_waic <- loo_compare(fit_normal1, fit_poisson1, fit_poisson2,
                         criterion = "waic")
print(comp_waic, simplify = FALSE, digits = 3)
```

```{r}
comp_waic %>%
  as.data.frame() %>%
  select(waic, elpd_diff, se_diff) %>%
  mutate(ll_diff = elpd_diff  + qnorm(0.025) * se_diff,
         ul_diff = elpd_diff  + qnorm(0.975) * se_diff)
```

## Conclusie

Zowel o.b.v. de PPC als de verschillende model selectie criteria, kunnen we besluiten dat het tweede Poisson model met random intercepts het best past bij de data.
In principe konden we dit ook verwachten op basis van onze eigen intuïtie en het design van de studie, nl. het gebruik van de Poisson distributie om aantallen te modelleren en het gebruik van random intercepts om te controleren voor een hiërarchisch design (habitats genest in sites).

# Resultaten finale model

https://mjskay.github.io/tidybayes/articles/tidy-brms.html

```{r}
fit_poisson2 %>%
  gather_draws(b_Intercept, b_habitatForest, ndraws = 1000, seed = 123) %>%
  group_by(.variable) %>%
  summarise(min = min(.value),
            q_05 = quantile(.value, probs = 0.05),
            gemiddelde = mean(.value),
            mediaan = median(.value),
            q_95 = quantile(.value, probs = 0.95),
            max = max(.value))
```

Handige functies zijn ook `median_qi()`, `mean_qi()` ...  na `gather_draws()` in plaats van `group_by()` en `summarise()`.

We zouden graag het geschatte aantal soorten visualiseren per habitattype.
Het gemiddeld aantal soorten in moerassen volgens ons model is $\exp(\beta_0)$ en in bossen $\exp(\beta_0+\beta_1)$.
We tonen de posterior distributions met de posterior mediaan en 60 en 90 % credible intervals.

```{r}
fit_poisson2 %>%
  spread_draws(b_Intercept, b_habitatForest, ndraws = 1000, seed = 123) %>%
  mutate(bog = exp(b_Intercept),
         forest = exp(b_Intercept + b_habitatForest)) %>%
  pivot_longer(cols = c("bog", "forest"), names_to = "habitat", 
               values_to = "sp_rich") %>%
  ggplot(aes(y = sp_rich, x = habitat)) +
    stat_eye(point_interval = "median_qi", .width = c(0.6, 0.9)) +
    scale_y_continuous(limits = c(0, NA))
```

We zien een duidelijk verschil in aantal soorten tussen beide habitats.
Is er een significant verschil tussen het aantal soorten in moerassen en bossen?
We testen de hypothese

$$
\exp(\beta_0) = \exp(\beta_0+\beta_1)\\
\Rightarrow \beta_0 = \beta_0 + \beta_1\\
\Rightarrow \beta_1 = 0\\
$$

```{r}
hyp <- hypothesis(fit_poisson2, "habitatForest = 0", alpha = 0.05)
hyp
```

```{r}
plot(hyp)
```

We sorteren de random effecten van de sites.

```{r}
# Get mean of sd of random effects 
sd_mean <- fit_poisson2 %>%
  spread_draws(sd_site__Intercept, ndraws = 1000, seed = 123) %>%
  summarise(mean_sd = mean(sd_site__Intercept)) %>%
  pull()

# Get random effects and plot
fit_poisson2 %>%
  spread_draws(r_site[site,], ndraws = 1000, seed = 123) %>%
  ungroup() %>%
  mutate(site = reorder(site, r_site)) %>%
  ggplot(aes(x = r_site, y = site)) +
    geom_vline(xintercept = 0, color = "darkgrey", linewidth = 1) +
    geom_vline(xintercept = c(sd_mean * qnorm(0.025), sd_mean * qnorm(0.975)),
               color = "darkgrey", linetype = 2) +
    stat_halfeye(point_interval = "median_qi", .width = 0.9, size = 2/3,
                 fill = "cornflowerblue")
```
